{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Colibri Core Python Tutorial: Efficiently working with n-grams, skipgrams and flexgrams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*by Maarten van Gompel, Radboud University Nijmegen*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial will show you how to work with Colibri Core's Python API, a tool for Natural Language Processing. It is assumed that you have already read the Colibri Core documentation, followed the installation instructions, and are familiar its purpose and concepts. The documentation also provides an API reference for all the Python classes and method. This tutorial is in the form of a Python Notebook, allowing you to interactively participate. Press ``shift+enter`` in code field to evaluate it.\n",
      "\n",
      "Colibri Core is written in C++ and the Python binding is writting in Cython. This offers the advantage of native-speed and memory efficiency, combined with the ease of a high-level pythonic interface. We will be using Python 3 here, but Colibri Core can also work with Python 2.7."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We obviously start our adventure with an import of colibricore, so make sure you installed it properly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import colibricore\n",
      "\n",
      "TMPDIR = \"/tmp/\" #this is where we'll store intermediate files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class encoding/decoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To give us something to work with, we will take an excerpt of Shakespeare's Hamlet as our corpus text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpustext = \"\"\"To be, or not to be, that is the question\n",
      "Whether 'tis Nobler in the mind to suffer\n",
      "The Slings and Arrows of outrageous Fortune,\n",
      "Or to take Arms against a Sea of troubles,\n",
      "And by opposing end them? To die, to sleep\n",
      "No more; and by a sleep, to say we end\n",
      "The Heart-ache, and the thousand Natural shocks\n",
      "That Flesh is heir to? 'Tis a consummation\n",
      "Devoutly to be wished. To die, to sleep,\n",
      "To sleep, perchance to Dream; Aye, there's the rub,\n",
      "For in that sleep of death, what dreams may come,\n",
      "When we have shuffled off this mortal coil,\n",
      "Must give us pause. There's the respect\n",
      "That makes Calamity of so long life:\n",
      "For who would bear the Whips and Scorns of time,\n",
      "Th' Oppressor's wrong, the proud man's Contumely,\n",
      "The pangs of despised Love, the Law\u2019s delay,\n",
      "The insolence of Office, and the Spurns\n",
      "That patient merit of the unworthy takes,\n",
      "When he himself might his Quietus make\n",
      "With a bare Bodkin? Who would these Fardels bear,\n",
      "To grunt and sweat under a weary life,\n",
      "But that the dread of something after death,\n",
      "The undiscovered Country, from whose bourn\n",
      "No Traveler returns, Puzzles the will,\n",
      "And makes us rather bear those ills we have,\n",
      "Than fly to others that we know not of.\n",
      "Thus Conscience does make Cowards of us all,\n",
      "And thus the Native hue of Resolution\n",
      "Is sicklied o'er, with the pale cast of Thought,\n",
      "And enterprises of great pitch and moment,\n",
      "With this regard their Currents turn awry,\n",
      "And lose the name of Action. Soft you now,\n",
      "The fair Ophelia. Nymph, in all thy Orisons\n",
      "Be all my sins remembered\"\"\"\n",
      "\n",
      "#first we do some very rudimentary tokenisation\n",
      "# Yes, I realise this is a very stupid way ;)\n",
      "corpustext = corpustext.replace(',',' ,')\n",
      "corpustext = corpustext.replace('.',' .')\n",
      "corpustext = corpustext.replace(':',' :')\n",
      "\n",
      "\n",
      "corpusfile_plaintext = TMPDIR + \"hamlet.txt\"\n",
      "\n",
      "with open(corpusfile_plaintext,'w',encoding='utf-8') as f:\n",
      "    f.write(corpustext)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To work with this data with Colibri Core. We need to *class encode* it, assigning integer values to each word type. Using Python, a class encoder is built as follows:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classfile = TMPDIR + \"hamlet.colibri.cls\"\n",
      "\n",
      "#Instantiate class encoder\n",
      "classencoder = colibricore.ClassEncoder()\n",
      "\n",
      "#Build classes\n",
      "classencoder.build(corpusfile_plaintext)\n",
      "\n",
      "#Save class file\n",
      "classencoder.save(classfile)\n",
      "\n",
      "print(\"Encoded \", len(classencoder), \" classes, well done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Encoded  184  classes, well done!\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a class encoder we can encode our corpus, producing a new encoded file (which tends to be about 50% compressed compared to the original):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpusfile = TMPDIR + \"hamlet.colibri.dat\" #this will be the encoded corpus file\n",
      "classencoder.encodefile(corpusfile_plaintext, corpusfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To check whether that worked as planned, we will construct a Class Decoder, load our class file, and decode the corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load class decoder from the classfile we just made\n",
      "classdecoder = colibricore.ClassDecoder(classfile)\n",
      "\n",
      "#Decode corpus data\n",
      "decoded = classdecoder.decodefile(corpusfile)\n",
      "\n",
      "#Show\n",
      "print(decoded)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be , or not to be , that is the question\n",
        "Whether 'tis Nobler in the mind to suffer\n",
        "The Slings and Arrows of outrageous Fortune ,\n",
        "Or to take Arms against a Sea of troubles ,\n",
        "And by opposing end them? To die , to sleep\n",
        "No more; and by a sleep , to say we end\n",
        "The Heart-ache , and the thousand Natural shocks\n",
        "That Flesh is heir to? 'Tis a consummation\n",
        "Devoutly to be wished . To die , to sleep ,\n",
        "To sleep , perchance to Dream; Aye , there's the rub ,\n",
        "For in that sleep of death , what dreams may come ,\n",
        "When we have shuffled off this mortal coil ,\n",
        "Must give us pause . There's the respect\n",
        "That makes Calamity of so long life :\n",
        "For who would bear the Whips and Scorns of time ,\n",
        "Th' Oppressor's wrong , the proud man's Contumely ,\n",
        "The pangs of despised Love , the Law\u2019s delay ,\n",
        "The insolence of Office , and the Spurns\n",
        "That patient merit of the unworthy takes ,\n",
        "When he himself might his Quietus make\n",
        "With a bare Bodkin? Who would these Fardels bear ,\n",
        "To grunt and sweat under a weary life ,\n",
        "But that the dread of something after death ,\n",
        "The undiscovered Country , from whose bourn\n",
        "No Traveler returns , Puzzles the will ,\n",
        "And makes us rather bear those ills we have ,\n",
        "Than fly to others that we know not of .\n",
        "Thus Conscience does make Cowards of us all ,\n",
        "And thus the Native hue of Resolution\n",
        "Is sicklied o'er , with the pale cast of Thought ,\n",
        "And enterprises of great pitch and moment ,\n",
        "With this regard their Currents turn awry ,\n",
        "And lose the name of Action . Soft you now ,\n",
        "The fair Ophelia . Nymph , in all thy Orisons\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Playing with patterns"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a class encoder and decoder, we can toy around with the most basic units in Colibri Core: **patterns**. These are using for n-grams, skipgrams, flexgrams and any kind of test. You would basically use an instance of ``Pattern`` where you'd normally use a string, as Patterns are much smaller in memory. Let's build a pattern from a string using the classencoder, note that we will only be able to use words that are known by the class encoder:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Build a pattern from a string, using the class encoder\n",
      "p = classencoder.buildpattern(\"To be or not to be\")\n",
      "\n",
      "#To print it we need the decoder\n",
      "print(p.tostring(classdecoder))\n",
      "print(len(p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be or not to be\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Iteration over a pattern will produce all the tokens that it is made up of. Note that the concepts of characters is gone from patterns! As a consequence, the ability to lowercase or uppercase text is also lost."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Iterate over the token in a pattern, each token will be a Pattern instance\n",
      "\n",
      "for token in p:\n",
      "    print(token.tostring(classdecoder))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or\n",
        "not\n",
        "to\n",
        "be\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Extracting subpatterns by offset\n",
      "\n",
      "#Get first token\n",
      "print(p[0].tostring(classdecoder))\n",
      "\n",
      "#Get last token\n",
      "print(p[-1].tostring(classdecoder))\n",
      "\n",
      "#Get slice\n",
      "print(p[2:4].tostring(classdecoder))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or not\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a pattern, we can now very easily extract all n-grams in it, one of the most common NLP tasks:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's get all bigrams\n",
      "for ngram in p.ngrams(2):\n",
      "    print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be\n",
        "be or\n",
        "or not\n",
        "not to\n",
        "to be\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#or all n-grams:\n",
      "for ngram in p.ngrams():\n",
      "    print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or\n",
        "not\n",
        "to\n",
        "be\n",
        "To be\n",
        "be or\n",
        "or not\n",
        "not to\n",
        "to be\n",
        "To be or\n",
        "be or not\n",
        "or not to\n",
        "not to be\n",
        "To be or not\n",
        "be or not to\n",
        "or not to be\n",
        "To be or not to\n",
        "be or not to be\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#or particular ngrams, such as unigrams up to trigrams:\n",
      "for ngram in p.ngrams(1,3):\n",
      "    print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or\n",
        "not\n",
        "to\n",
        "be\n",
        "To be\n",
        "be or\n",
        "or not\n",
        "not to\n",
        "to be\n",
        "To be or\n",
        "be or not\n",
        "or not to\n",
        "not to be\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The ``in`` operator can be used to check if a token **OR** ngram is part of a pattern"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#token\n",
      "p2 = classencoder.buildpattern(\"be\")\n",
      "print(p2 in p)\n",
      "\n",
      "#ngram\n",
      "p3 = classencoder.buildpattern(\"or not\")\n",
      "print(p3 in p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The follow snippet is here just to prove that our Pattern representation is usually smaller than a string representation, and offers a sneak peek under the hood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(bytes(p), len(bytes(p)))\n",
      "print(b\"To be or not to be\", len(b\"To be or not to be\"))\n",
      "len(bytes(p)) < len(b\"To be or not to be\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "b'\\x01\\x0e\\x01\\x16\\x01\\x83\\x01\\x1e\\x01\\t\\x01\\x16' 12\n",
        "b'To be or not to be' 18\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Reading a corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to read an entire corpus, we can use the ``IndexedCorpus`` class. This we can use, for example, if we are merely interested in moving a sliding window over our data and extracting n-grams without counting or storing them:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpusdata = colibricore.IndexedCorpus(corpusfile) #encoded data, will be loaded into memory entirely\n",
      "\n",
      "for sentence in corpusdata.sentences(): #will return a Pattern per sentence (generator)\n",
      "    for trigram in sentence.ngrams(3):\n",
      "        print(trigram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be ,\n",
        "be , or\n",
        ", or not\n",
        "or not to\n",
        "not to be\n",
        "to be ,\n",
        "be , that\n",
        ", that is\n",
        "that is the\n",
        "is the question\n",
        "Whether 'tis Nobler\n",
        "'tis Nobler in\n",
        "Nobler in the\n",
        "in the mind\n",
        "the mind to\n",
        "mind to suffer\n",
        "The Slings and\n",
        "Slings and Arrows\n",
        "and Arrows of\n",
        "Arrows of outrageous\n",
        "of outrageous Fortune\n",
        "outrageous Fortune ,\n",
        "Or to take\n",
        "to take Arms\n",
        "take Arms against\n",
        "Arms against a\n",
        "against a Sea\n",
        "a Sea of\n",
        "Sea of troubles\n",
        "of troubles ,\n",
        "And by opposing\n",
        "by opposing end\n",
        "opposing end them?\n",
        "end them? To\n",
        "them? To die\n",
        "To die ,\n",
        "die , to\n",
        ", to sleep\n",
        "No more; and\n",
        "more; and by\n",
        "and by a\n",
        "by a sleep\n",
        "a sleep ,\n",
        "sleep , to\n",
        ", to say\n",
        "to say we\n",
        "say we end\n",
        "The Heart-ache ,\n",
        "Heart-ache , and\n",
        ", and the\n",
        "and the thousand\n",
        "the thousand Natural\n",
        "thousand Natural shocks\n",
        "That Flesh is\n",
        "Flesh is heir\n",
        "is heir to?\n",
        "heir to? 'Tis\n",
        "to? 'Tis a\n",
        "'Tis a consummation\n",
        "Devoutly to be\n",
        "to be wished\n",
        "be wished .\n",
        "wished . To\n",
        ". To die\n",
        "To die ,\n",
        "die , to\n",
        ", to sleep\n",
        "to sleep ,\n",
        "To sleep ,\n",
        "sleep , perchance\n",
        ", perchance to\n",
        "perchance to Dream;\n",
        "to Dream; Aye\n",
        "Dream; Aye ,\n",
        "Aye , there's\n",
        ", there's the\n",
        "there's the rub\n",
        "the rub ,\n",
        "For in that\n",
        "in that sleep\n",
        "that sleep of\n",
        "sleep of death\n",
        "of death ,\n",
        "death , what\n",
        ", what dreams\n",
        "what dreams may\n",
        "dreams may come\n",
        "may come ,\n",
        "When we have\n",
        "we have shuffled\n",
        "have shuffled off\n",
        "shuffled off this\n",
        "off this mortal\n",
        "this mortal coil\n",
        "mortal coil ,\n",
        "Must give us\n",
        "give us pause\n",
        "us pause .\n",
        "pause . There's\n",
        ". There's the\n",
        "There's the respect\n",
        "That makes Calamity\n",
        "makes Calamity of\n",
        "Calamity of so\n",
        "of so long\n",
        "so long life\n",
        "long life :\n",
        "For who would\n",
        "who would bear\n",
        "would bear the\n",
        "bear the Whips\n",
        "the Whips and\n",
        "Whips and Scorns\n",
        "and Scorns of\n",
        "Scorns of time\n",
        "of time ,\n",
        "Th' Oppressor's wrong\n",
        "Oppressor's wrong ,\n",
        "wrong , the\n",
        ", the proud\n",
        "the proud man's\n",
        "proud man's Contumely\n",
        "man's Contumely ,\n",
        "The pangs of\n",
        "pangs of despised\n",
        "of despised Love\n",
        "despised Love ,\n",
        "Love , the\n",
        ", the Law\u2019s\n",
        "the Law\u2019s delay\n",
        "Law\u2019s delay ,\n",
        "The insolence of\n",
        "insolence of Office\n",
        "of Office ,\n",
        "Office , and\n",
        ", and the\n",
        "and the Spurns\n",
        "That patient merit\n",
        "patient merit of\n",
        "merit of the\n",
        "of the unworthy\n",
        "the unworthy takes\n",
        "unworthy takes ,\n",
        "When he himself\n",
        "he himself might\n",
        "himself might his\n",
        "might his Quietus\n",
        "his Quietus make\n",
        "With a bare\n",
        "a bare Bodkin?\n",
        "bare Bodkin? Who\n",
        "Bodkin? Who would\n",
        "Who would these\n",
        "would these Fardels\n",
        "these Fardels bear\n",
        "Fardels bear ,\n",
        "To grunt and\n",
        "grunt and sweat\n",
        "and sweat under\n",
        "sweat under a\n",
        "under a weary\n",
        "a weary life\n",
        "weary life ,\n",
        "But that the\n",
        "that the dread\n",
        "the dread of\n",
        "dread of something\n",
        "of something after\n",
        "something after death\n",
        "after death ,\n",
        "The undiscovered Country\n",
        "undiscovered Country ,\n",
        "Country , from\n",
        ", from whose\n",
        "from whose bourn\n",
        "No Traveler returns\n",
        "Traveler returns ,\n",
        "returns , Puzzles\n",
        ", Puzzles the\n",
        "Puzzles the will\n",
        "the will ,\n",
        "And makes us\n",
        "makes us rather\n",
        "us rather bear\n",
        "rather bear those\n",
        "bear those ills\n",
        "those ills we\n",
        "ills we have\n",
        "we have ,\n",
        "Than fly to\n",
        "fly to others\n",
        "to others that\n",
        "others that we\n",
        "that we know\n",
        "we know not\n",
        "know not of\n",
        "not of .\n",
        "Thus Conscience does\n",
        "Conscience does make\n",
        "does make Cowards\n",
        "make Cowards of\n",
        "Cowards of us\n",
        "of us all\n",
        "us all ,\n",
        "And thus the\n",
        "thus the Native\n",
        "the Native hue\n",
        "Native hue of\n",
        "hue of Resolution\n",
        "Is sicklied o'er\n",
        "sicklied o'er ,\n",
        "o'er , with\n",
        ", with the\n",
        "with the pale\n",
        "the pale cast\n",
        "pale cast of\n",
        "cast of Thought\n",
        "of Thought ,\n",
        "And enterprises of\n",
        "enterprises of great\n",
        "of great pitch\n",
        "great pitch and\n",
        "pitch and moment\n",
        "and moment ,\n",
        "With this regard\n",
        "this regard their\n",
        "regard their Currents\n",
        "their Currents turn\n",
        "Currents turn awry\n",
        "turn awry ,\n",
        "And lose the\n",
        "lose the name\n",
        "the name of\n",
        "name of Action\n",
        "of Action .\n",
        "Action . Soft\n",
        ". Soft you\n",
        "Soft you now\n",
        "you now ,\n",
        "The fair Ophelia\n",
        "fair Ophelia .\n",
        "Ophelia . Nymph\n",
        ". Nymph ,\n",
        "Nymph , in\n",
        ", in all\n",
        "in all thy\n",
        "all thy Orisons\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now you may be very tempted to start storing and counting n-grams this way, but **don't**. This method is only suitable for iterating and quickly discarding the ngrams. Colibri core has facilities to deal with storing and counting far more efficiently, these are *pattern models* which we will discuss in the next section."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First some more about ``IndexedCorpus``. We can also obtain any pattern using its index, a ``(sentence,token)`` tuple:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigram = corpusdata[(2,3)]\n",
      "print(unigram.tostring(classdecoder))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A slice syntax is also supported, but may never cross line/sentence boundaries. As is customary in Python, the last index is non-inclusive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram = corpusdata[(2,3):(2,8)]\n",
      "print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in the mind to suffer\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of sentences and the length of each sentence can be extracted as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentencecount = corpusdata.sentencecount()\n",
      "for i in range(1, sentencecount+1): #note the 1..+1 range, sentences are 1-indexed (whereas tokens are 0-indexed)\n",
      "    print(\"Length of sentence \" + str(i) + \":\", corpusdata.sentencelength(i))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Length of sentence 1: 12\n",
        "Length of sentence 2: 8\n",
        "Length of sentence 3: 8\n",
        "Length of sentence 4: 10\n",
        "Length of sentence 5: 10\n",
        "Length of sentence 6: 11\n",
        "Length of sentence 7: 8\n",
        "Length of sentence 8: 8\n",
        "Length of sentence 9: 11\n",
        "Length of sentence 10: 12\n",
        "Length of sentence 11: 12\n",
        "Length of sentence 12: 9\n",
        "Length of sentence 13: 8\n",
        "Length of sentence 14: 8\n",
        "Length of sentence 15: 11\n",
        "Length of sentence 16: 9\n",
        "Length of sentence 17: 10\n",
        "Length of sentence 18: 8\n",
        "Length of sentence 19: 8\n",
        "Length of sentence 20: 7\n",
        "Length of sentence 21: 10\n",
        "Length of sentence 22: 9\n",
        "Length of sentence 23: 9\n",
        "Length of sentence 24: 7\n",
        "Length of sentence 25: 8\n",
        "Length of sentence 26: 10\n",
        "Length of sentence 27: 10\n",
        "Length of sentence 28: 9\n",
        "Length of sentence 29: 7\n",
        "Length of sentence 30: 11\n",
        "Length of sentence 31: 8\n",
        "Length of sentence 32: 8\n",
        "Length of sentence 33: 11\n",
        "Length of sentence 34: 10\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pattern Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it's time to build our first pattern model on the Hamlet excerpt. We will extract all patterns occurring at least twice and with maximum length 8."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set the options\n",
      "options = colibricore.PatternModelOptions(mintokens=2,maxlength=8)\n",
      "\n",
      "#Instantiate an empty unindexed model \n",
      "model = colibricore.UnindexedPatternModel()\n",
      "\n",
      "#Train it on our corpus file (class-encoded data, not plain text)\n",
      "model.train(corpusfile, options)\n",
      "\n",
      "print(\"Found \" , len(model), \" patterns:\")\n",
      "\n",
      "#Let's see what patterns are in our model (the order will be 'random')\n",
      "for pattern in model:\n",
      "    print(pattern.tostring(classdecoder))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found  54  patterns:\n",
        "To die , to sleep\n",
        "To die , to\n",
        ", and the\n",
        ", to sleep\n",
        "die , to\n",
        "To die ,\n",
        ", the\n",
        "and the\n",
        ", and\n",
        "sleep ,\n",
        "to sleep\n",
        "die , to sleep\n",
        ", to\n",
        "die ,\n",
        "we have\n",
        "to be\n",
        "To die\n",
        "be ,\n",
        "all\n",
        "With\n",
        "we\n",
        "sleep\n",
        "That\n",
        "by\n",
        "a\n",
        "make\n",
        "die\n",
        "end\n",
        "to\n",
        "And\n",
        ",\n",
        "be\n",
        "To\n",
        "would\n",
        "this\n",
        "death ,\n",
        "No\n",
        "not\n",
        ".\n",
        "us\n",
        "The\n",
        "For\n",
        "of\n",
        "death\n",
        "bear\n",
        "When\n",
        "and\n",
        "have\n",
        "is\n",
        "in\n",
        "the\n",
        "makes\n",
        "that\n",
        "life\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rather than just output the patterns, we of course now have the counts as well, let's output it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Models behave much alike to Python dictionaries:\n",
      "for pattern, count in model.items():\n",
      "    print(pattern.tostring(classdecoder), count)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To die , to sleep 2\n",
        "To die , to 2\n",
        ", and the 2\n",
        ", to sleep 2\n",
        "die , to 2\n",
        "To die , 2\n",
        ", the 2\n",
        "and the 2\n",
        ", and 2\n",
        "sleep , 3\n",
        "to sleep 2\n",
        "die , to sleep 2\n",
        ", to 3\n",
        "die , 2\n",
        "we have 2\n",
        "to be 2\n",
        "To die 2\n",
        "be , 2\n",
        "all 2\n",
        "With 2\n",
        "we 4\n",
        "sleep 5\n",
        "That 3\n",
        "by 2\n",
        "a 5\n",
        "make 2\n",
        "die 2\n",
        "end 2\n",
        "to 9\n",
        "And 5\n",
        ", 36\n",
        "be 3\n",
        "To 5\n",
        "would 2\n",
        "this 2\n",
        "death , 2\n",
        "No 2\n",
        "not 2\n",
        ". 5\n",
        "us 3\n",
        "The 6\n",
        "For 2\n",
        "of 15\n",
        "death 2\n",
        "bear 3\n",
        "When 2\n",
        "and 7\n",
        "have 2\n",
        "is 2\n",
        "in 3\n",
        "the 15\n",
        "makes 2\n",
        "that 4\n",
        "life 2\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also query specific patterns:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "querypattern = classencoder.buildpattern(\"sleep\")\n",
      "\n",
      "print(\"How much sleep?\")\n",
      "print(model[querypattern])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "How much sleep?\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Like dictionaries, unknown patterns will trigger a KeyError\n",
      "querypattern = classencoder.buildpattern(\"insolence\")\n",
      "\n",
      "print(\"How much insolence?\")\n",
      "try:\n",
      "    print(model[querypattern])\n",
      "except KeyError:\n",
      "    print(\"Nope, KeyError, no such pattern in model..\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "How much insolence?\n",
        "Nope, KeyError, no such pattern in model..\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can check whether a pattern is in a model in the usual pythonic fashion:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if querypattern in model:\n",
      "    print(\"Insolence in model!\")\n",
      "else:\n",
      "    print(\"No insolence in model!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No insolence in model!\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rather than the absolute counts, we can get the frequency of a pattern *within its type and class*. For example the frequency of a bigram amongst all bigrams:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "querypattern = classencoder.buildpattern(\"and the\")\n",
      "\n",
      "print(model.frequency(querypattern))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.07692307692307693\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To analyse the distribution of occurrences, we can extract a histogram from our model as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for occurrencecount, frequency in model.histogram():\n",
      "    print(occurrencecount , \" occurrences by \", frequency , \"patterns\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2  occurrences by  34 patterns\n",
        "3  occurrences by  7 patterns\n",
        "4  occurrences by  2 patterns\n",
        "5  occurrences by  5 patterns\n",
        "6  occurrences by  1 patterns\n",
        "7  occurrences by  1 patterns\n",
        "9  occurrences by  1 patterns\n",
        "15  occurrences by  2 patterns\n",
        "36  occurrences by  1 patterns\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have a model, we can save it to file, to reload later, loading is much faster than training:\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patternmodelfile = TMPDIR + \"hamlet.colibri.patternmodel\"\n",
      "\n",
      "model.write(patternmodelfile)\n",
      "\n",
      "#and reload just to show we can:\n",
      "model = colibricore.UnindexedPatternModel(patternmodelfile, options)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unindexed models are much smaller in memory than indexed models, but their functionality is also limited. Let's take a look at *indexed models*. Indexed models keep a *forward index* to all locations in the original corpus where patterns occur. The references are 2-tuples in the form ``(sentence,token)``, where ``sentence`` is 1-indexed and ``token`` is 0-indexed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set the options\n",
      "options = colibricore.PatternModelOptions(mintokens=2,maxlength=8,doreverseindex=True)\n",
      "\n",
      "#Instantiate an empty indexed model \n",
      "model = colibricore.IndexedPatternModel()\n",
      "\n",
      "#Train it on our corpus file (class-encoded data, not plain text)\n",
      "model.train(corpusfile, options)\n",
      "\n",
      "print(\"Found \" , len(model), \" patterns:\")\n",
      "\n",
      "#Let's see what patterns are in our model (the order will be 'random')\n",
      "for pattern, indices in model.items():\n",
      "    print(pattern.tostring(classdecoder),end=\" \")\n",
      "    for index in indices:\n",
      "        print(index,end=\" \") #(sentence,token) tuple, sentences start with 1, tokens with 0\n",
      "    print()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found  54  patterns:\n",
        "To die , to sleep (5, 5) (9, 5) \n",
        "To die , to (5, 5) (9, 5) \n",
        ", and the (7, 2) (18, 4) \n",
        ", to sleep (5, 7) (9, 7) \n",
        "die , to (5, 6) (9, 6) \n",
        "To die , (5, 5) (9, 5) \n",
        ", the (16, 3) (17, 5) \n",
        "and the (7, 3) (18, 5) \n",
        ", and (7, 2) (18, 4) \n",
        "sleep , (6, 5) (9, 9) (10, 1) \n",
        "to sleep (5, 8) (9, 8) \n",
        "die , to sleep (5, 6) (9, 6) \n",
        ", to (5, 7) (6, 6) (9, 7) \n",
        "die , (5, 6) (9, 6) \n",
        "we have (12, 1) (26, 7) \n",
        "to be (1, 5) (9, 1) \n",
        "To die (5, 5) (9, 5) \n",
        "be , (1, 1) (1, 6) \n",
        "all (28, 7) (34, 7) \n",
        "With (21, 0) (32, 0) \n",
        "we (6, 9) (12, 1) (26, 7) (27, 5) \n",
        "sleep (5, 9) (6, 5) (9, 9) (10, 1) (11, 3) \n",
        "That (8, 0) (14, 0) (19, 0) \n",
        "by (5, 1) (6, 3) \n",
        "a (4, 5) (6, 4) (8, 6) (21, 1) (22, 5) \n",
        "make (20, 6) (28, 3) \n",
        "die (5, 6) (9, 6) \n",
        "end (5, 3) (6, 10) \n",
        "to (1, 5) (2, 6) (4, 1) (5, 8) (6, 7) (9, 1) (9, 8) (10, 4) (27, 2) \n",
        "And (5, 0) (26, 0) (29, 0) (31, 0) (33, 0) \n",
        ", (1, 2) (1, 7) (3, 7) (4, 9) (5, 7) (6, 6) (7, 2) (9, 7) (9, 10) (10, 2) (10, 7) (10, 11) (11, 6) (11, 11) (12, 8) (15, 10) (16, 3) (16, 8) (17, 5) (17, 9) (18, 4) (19, 7) (21, 9) (22, 8) (23, 8) (24, 3) (25, 3) (25, 7) (26, 9) (28, 8) (30, 3) (30, 10) (31, 7) (32, 7) (33, 10) (34, 5) \n",
        "be (1, 1) (1, 6) (9, 2) \n",
        "To (1, 0) (5, 5) (9, 5) (10, 0) (22, 0) \n",
        "would (15, 2) (21, 5) \n",
        "this (12, 5) (32, 1) \n",
        "death , (11, 5) (23, 7) \n",
        "No (6, 0) (25, 0) \n",
        "not (1, 4) (27, 7) \n",
        ". (9, 4) (13, 4) (27, 9) (33, 6) (34, 3) \n",
        "us (13, 2) (26, 2) (28, 6) \n",
        "The (3, 0) (7, 0) (17, 0) (18, 0) (24, 0) (34, 0) \n",
        "For (11, 0) (15, 0) \n",
        "of (3, 4) (4, 7) (11, 4) (14, 3) (15, 8) (17, 2) (18, 2) (19, 3) (23, 4) (27, 8) (28, 5) (29, 5) (30, 8) (31, 2) (33, 4) \n",
        "death (11, 5) (23, 7) \n",
        "bear (15, 3) (21, 8) (26, 4) \n",
        "When (12, 0) (20, 0) \n",
        "and (3, 2) (6, 2) (7, 3) (15, 6) (18, 5) (22, 2) (31, 5) \n",
        "have (12, 2) (26, 8) \n",
        "is (1, 9) (8, 2) \n",
        "in (2, 3) (11, 1) (34, 6) \n",
        "the (1, 10) (2, 4) (7, 4) (10, 9) (13, 6) (15, 4) (16, 4) (17, 6) (18, 6) (19, 4) (23, 2) (25, 5) (29, 2) (30, 5) (33, 2) \n",
        "makes (14, 1) (26, 1) \n",
        "that (1, 8) (11, 2) (23, 1) (27, 4) \n",
        "life (14, 6) (22, 7) \n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One interesting feature we can get from indexed models, is coverage information. This shows how many of the tokens in the original corpus data are covered by a particular pattern."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "querypattern = classencoder.buildpattern(\"and the\")\n",
      "\n",
      "print(model.coverage(querypattern))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.012698412698412698\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some numbers on the original corpus data can be obtained from the model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Total amount of tokens in the corpus data:\" , model.tokens() )\n",
      "print(\"Total amount of word types in the corpus data:\" , model.types() )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total amount of tokens in the corpus data: 315\n",
        "Total amount of word types in the corpus data: 180\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We trained the corpus with the option ``doreverseindex=True``, when used with indexed pattern models this constructs a *reverse index* from the forward index. This reverse index allows you to look what patterns *begin* at a particular location, expressed as a ``(sentence, token)`` tuple in the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Patterns at (1,5): \")\n",
      "for pattern in model.getreverseindex( (1,5) ):\n",
      "    print(pattern.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Patterns at (1,5): \n",
        "to\n",
        "to be\n",
        "to be ,\n",
        "to be , that\n",
        "to be , that is\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also use this to easily get all patterns in a sentence:\n",
      "    \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Patterns in first sentence\")\n",
      "for (sentence, token), pattern in model.getreverseindex_bysentence(1):\n",
      "    print(sentence,token, \" -- \", pattern.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Patterns in first sentence\n",
        "1 0  --  To\n",
        "1 0  --  To be\n",
        "1 0  --  To be ,\n",
        "1 0  --  To be , or\n",
        "1 0  --  To be , or not\n",
        "1 1  --  be\n",
        "1 1  --  be ,\n",
        "1 1  --  be , or\n",
        "1 1  --  be , or not\n",
        "1 1  --  be , or not to\n",
        "1 2  --  ,\n",
        "1 2  --  , or\n",
        "1 2  --  , or not\n",
        "1 2  --  , or not to\n",
        "1 2  --  , or not to be\n",
        "1 3  --  or\n",
        "1 3  --  or not\n",
        "1 3  --  or not to\n",
        "1 3  --  or not to be\n",
        "1 3  --  or not to be ,\n",
        "1 4  --  not\n",
        "1 4  --  not to\n",
        "1 4  --  not to be\n",
        "1 4  --  not to be ,\n",
        "1 4  --  not to be , that\n",
        "1 5  --  to\n",
        "1 5  --  to be\n",
        "1 5  --  to be ,\n",
        "1 5  --  to be , that\n",
        "1 5  --  to be , that is\n",
        "1 6  --  be\n",
        "1 6  --  be ,\n",
        "1 6  --  be , that\n",
        "1 6  --  be , that is\n",
        "1 6  --  be , that is the\n",
        "1 7  --  ,\n",
        "1 7  --  , that\n",
        "1 7  --  , that is\n",
        "1 7  --  , that is the\n",
        "1 7  --  , that is the question\n",
        "1 8  --  that\n",
        "1 8  --  that is\n",
        "1 8  --  that is the\n",
        "1 8  --  that is the question\n",
        "1 9  --  is\n",
        "1 9  --  is the\n",
        "1 9  --  is the question\n",
        "1 10  --  the\n",
        "1 10  --  the question\n",
        "1 11  --  question\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is easy to iterate over all indices in the reverse index:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for ref in model.reverseindex():\n",
      "    print(ref, end=\" \")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 0) (1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (1, 6) (1, 7) (1, 8) (1, 9) (1, 10) (1, 11) (2, 0) (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (2, 6) (2, 7) (3, 0) (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (3, 6) (3, 7) (4, 0) (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (4, 6) (4, 7) (4, 8) (4, 9) (5, 0) (5, 1) (5, 2) (5, 3) (5, 4) (5, 5) (5, 6) (5, 7) (5, 8) (5, 9) (6, 0) (6, 1) (6, 2) (6, 3) (6, 4) (6, 5) (6, 6) (6, 7) (6, 8) (6, 9) (6, 10) (7, 0) (7, 1) (7, 2) (7, 3) (7, 4) (7, 5) (7, 6) (7, 7) (8, 0) (8, 1) (8, 2) (8, 3) (8, 4) (8, 5) (8, 6) (8, 7) (9, 0) (9, 1) (9, 2) (9, 3) (9, 4) (9, 5) (9, 6) (9, 7) (9, 8) (9, 9) (9, 10) (10, 0) (10, 1) (10, 2) (10, 3) (10, 4) (10, 5) (10, 6) (10, 7) (10, 8) (10, 9) (10, 10) (10, 11) (11, 0) (11, 1) (11, 2) (11, 3) (11, 4) (11, 5) (11, 6) (11, 7) (11, 8) (11, 9) (11, 10) (11, 11) (12, 0) (12, 1) (12, 2) (12, 3) (12, 4) (12, 5) (12, 6) (12, 7) (12, 8) (13, 0) (13, 1) (13, 2) (13, 3) (13, 4) (13, 5) (13, 6) (13, 7) (14, 0) (14, 1) (14, 2) (14, 3) (14, 4) (14, 5) (14, 6) (14, 7) (15, 0) (15, 1) (15, 2) (15, 3) (15, 4) (15, 5) (15, 6) (15, 7) (15, 8) (15, 9) (15, 10) (16, 0) (16, 1) (16, 2) (16, 3) (16, 4) (16, 5) (16, 6) (16, 7) (16, 8) (17, 0) (17, 1) (17, 2) (17, 3) (17, 4) (17, 5) (17, 6) (17, 7) (17, 8) (17, 9) (18, 0) (18, 1) (18, 2) (18, 3) (18, 4) (18, 5) (18, 6) (18, 7) (19, 0) (19, 1) (19, 2) (19, 3) (19, 4) (19, 5) (19, 6) (19, 7) (20, 0) (20, 1) (20, 2) (20, 3) (20, 4) (20, 5) (20, 6) (21, 0) (21, 1) (21, 2) (21, 3) (21, 4) (21, 5) (21, 6) (21, 7) (21, 8) (21, 9) (22, 0) (22, 1) (22, 2) (22, 3) (22, 4) (22, 5) (22, 6) (22, 7) (22, 8) (23, 0) (23, 1) (23, 2) (23, 3) (23, 4) (23, 5) (23, 6) (23, 7) (23, 8) (24, 0) (24, 1) (24, 2) (24, 3) (24, 4) (24, 5) (24, 6) (25, 0) (25, 1) (25, 2) (25, 3) (25, 4) (25, 5) (25, 6) (25, 7) (26, 0) (26, 1) (26, 2) (26, 3) (26, 4) (26, 5) (26, 6) (26, 7) (26, 8) (26, 9) (27, 0) (27, 1) (27, 2) (27, 3) (27, 4) (27, 5) (27, 6) (27, 7) (27, 8) (27, 9) (28, 0) (28, 1) (28, 2) (28, 3) (28, 4) (28, 5) (28, 6) (28, 7) (28, 8) (29, 0) (29, 1) (29, 2) (29, 3) (29, 4) (29, 5) (29, 6) (30, 0) (30, 1) (30, 2) (30, 3) (30, 4) (30, 5) (30, 6) (30, 7) (30, 8) (30, 9) (30, 10) (31, 0) (31, 1) (31, 2) (31, 3) (31, 4) (31, 5) (31, 6) (31, 7) (32, 0) (32, 1) (32, 2) (32, 3) (32, 4) (32, 5) (32, 6) (32, 7) (33, 0) (33, 1) (33, 2) (33, 3) (33, 4) (33, 5) (33, 6) (33, 7) (33, 8) (33, 9) (33, 10) (34, 0) (34, 1) (34, 2) (34, 3) (34, 4) (34, 5) (34, 6) (34, 7) (34, 8) (34, 9) "
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Actually, the reverse index, as returned by the ``reverseindex()`` method, is just an instance of ``IndexedCorpus``, which we already saw earlier."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Skipgrams and flexgrams and relations between patterns"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Skipgrams are n-grams with one or more *gaps* of a particular size. Flexgrams have a gap of dynamic size. Colibri Core can deal with both. Let's start with a new, and somewhat bigger, corpus. As the data in our previous example was too sparse to find any skipgrams. To that end, we will download Plato's *Republic*, this version is already tokenised and has one sentence per line, just as Colibri Core likes it: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib.request\n",
      "corpusfile_plato_plaintext = TMPDIR + \"republic.txt\"\n",
      "f = urllib.request.urlopen('http://lst.science.ru.nl/~proycon/republic.txt')\n",
      "with open(corpusfile_plato_plaintext,'wb') as of:\n",
      "    of.write(f.read())\n",
      "print(\"Downloaded to \" + corpusfile_plato_plaintext)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloaded to /tmp/republic.txt\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we create a class file and class encode the corpus, but because we may later on want to compare Shakespeare's Hamlet with Plato's Republic, we ensure that we use the same vocabulary. Note that it would have been better (more optimal classes, better compression) if we had built the original class encoder on both files right away, but you don't always have the luxury of foresight.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classfile_plato = TMPDIR + \"republic.colibri.cls\"\n",
      "corpusfile_plato  = TMPDIR + \"republic.colibri.dat\"\n",
      "\n",
      "#Build classes, re-using our classencoder from Hamlet! Let's reload it just for completion's sake\n",
      "classencoder = colibricore.ClassEncoder(TMPDIR + \"hamlet.colibri.cls\")\n",
      "\n",
      "#Now we will extend it by buildiing classes on Plato's data. If we had done this earlier, \n",
      "# we could have passed a list of filenames, ensuring more optimal encoding.\n",
      "classencoder.build(corpusfile_plato_plaintext)\n",
      "\n",
      "#Save new class file, this will be a superset of the original one.\n",
      "classencoder.save(classfile_plato)\n",
      "\n",
      "#Encode the corpus\n",
      "classencoder.encodefile(corpusfile_plato_plaintext, corpusfile_plato)\n",
      "\n",
      "#Load decoder because the old one will only handle Hamlet\n",
      "classdecoder = colibricore.ClassDecoder(classfile_plato)\n",
      "\n",
      "print(\"Done\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a proper class file and encoded corpus, we can build an indexed pattern model with skipgrams. Skipgrams can only be build most efficiently using indexed models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set the options, doskipgrams=True is the key to enabling skipgrams\n",
      "options = colibricore.PatternModelOptions(mintokens=2,maxlength=8,doreverseindex=True, doskipgrams=True)\n",
      "\n",
      "#Instantiate an empty indexed model \n",
      "model = colibricore.IndexedPatternModel()\n",
      "\n",
      "#Train it on our corpus file (class-encoded data, not plain text)\n",
      "model.train(corpusfile_plato, options)\n",
      "\n",
      "print(\"Found \" , len(model), \" patterns:\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found  84310  patterns:\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now how many of those patterns are skipgrams? We can find out ourselves by iterating over the patterns and checking their *category*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skipgrams = 0\n",
      "for pattern in model:\n",
      "    if pattern.category() == colibricore.Category.SKIPGRAM:\n",
      "        skipgrams += 1\n",
      "print(\"Found\",skipgrams,\" skipgrams\")\n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 7921  skipgrams\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, it is much faster to do this using the built-in ``filter()`` method, which can also be used to filter patterns above a certain occurrence threshold, we can constrain it to a specific type such as skipgrams, and to a specific length (third argument, not used here):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skipgrams = 0\n",
      "for pattern, occurrencecount in model.filter(0,colibricore.Category.SKIPGRAM): #the first parameter is the occurrence threshold\n",
      "    skipgrams += 1\n",
      "print(\"Found\",skipgrams,\" skipgrams\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 7921  skipgrams\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similar to ``filter()`` is the ``top()`` method, which we can use to extract the top patterns, let's get the top 20 of skipgrams. We will still need to relay it through a sorting function to get it in descending order:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for pattern, occurrencecount in sorted( model.top(20,colibricore.Category.SKIPGRAM), key=lambda x:x[1]*-1 ):\n",
      "    print(pattern.tostring(classdecoder), \" -- \", occurrencecount)\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the {*1*} of  --  3392\n",
        ", {*1*} the  --  1058\n",
        "of {*1*} ,  --  947\n",
        "the {*1*} ,  --  918\n",
        ", {*1*} said  --  857\n",
        "the {*2*} the  --  805\n",
        ", {*2*} ,  --  777\n",
        "the {*1*} of the  --  672\n",
        ", {*1*} ,  --  562\n",
        ", {*1*} is  --  536\n",
        ", {*2*} the  --  496\n",
        ", {*1*} said ,  --  492\n",
        "the {*1*} .  --  454\n",
        "I {*1*} ,  --  445\n",
        "of {*1*} .  --  395\n",
        "the {*1*} and  --  391\n",
        "he {*1*} .  --  389\n",
        "of {*1*} and  --  388\n",
        ", {*1*} he  --  386\n",
        "of {*2*} ,  --  379\n",
        ", {*2*} .  --  372\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The size of a skipgram gap is indicated by the number. We can create skipgrams from scratch using the same syntax with the classencoder:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skipgram = classencoder.buildpattern(\"To {*1*} or not to {*1*} is the question\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The consecutive non-gap parts of a skipgram can be obtained using the ``parts()`` method. The skipgram above consists of three parts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for part in skipgram.parts():\n",
      "    print(part.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "or not to\n",
        "is the question\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because an indexed model stores all the locations at which a pattern occurs, and a reverse index allows us to fill missing gaps, we can easily obtain all n-grams of which the skipgram is an abstraction:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's pick a common skipgram from the data:\n",
      "skipgram = classencoder.buildpattern(\"to the {*1*} of\")\n",
      "\n",
      "for ngram, occurrences in model.getinstances(skipgram):\n",
      "    print(ngram.tostring(classdecoder), \" -- occurring \", occurrences, \" times\" )\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "to the words of  -- occurring  2  times\n",
        "to the Republic of  -- occurring  2  times\n",
        "to the level of  -- occurring  2  times\n",
        "to the eye of  -- occurring  2  times\n",
        "to the importance of  -- occurring  2  times\n",
        "to the happiness of  -- occurring  2  times\n",
        "to the wants of  -- occurring  2  times\n",
        "to the sight of  -- occurring  3  times\n",
        "to the contemplation of  -- occurring  5  times\n",
        "to the good of  -- occurring  5  times\n",
        "to the voice of  -- occurring  2  times\n",
        "to the idea of  -- occurring  8  times\n",
        "to the class of  -- occurring  4  times\n",
        "to the end of  -- occurring  3  times\n",
        "to the law of  -- occurring  2  times\n",
        "to the terms of  -- occurring  2  times\n",
        "to the tale of  -- occurring  2  times\n",
        "to the vision of  -- occurring  2  times\n",
        "to the question of  -- occurring  2  times\n",
        "to the mind of  -- occurring  2  times\n",
        "to the rest of  -- occurring  4  times\n",
        "to the relation of  -- occurring  3  times\n",
        "to the conditions of  -- occurring  3  times\n",
        "to the plain of  -- occurring  2  times\n",
        "to the sum of  -- occurring  4  times\n",
        "to the knowledge of  -- occurring  3  times\n",
        "to the authority of  -- occurring  3  times\n",
        "to the devastation of  -- occurring  2  times\n",
        "to the number of  -- occurring  2  times\n",
        "to the injury of  -- occurring  3  times\n",
        "to the nature of  -- occurring  2  times\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The reverse is also possible, given an ngram we can find what skipgrams are abstractions, or *templates* of it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's pick a common skipgram from the data:\n",
      "ngram = classencoder.buildpattern(\"to the idea of\")\n",
      "\n",
      "for skipgram, occurrences in model.gettemplates(ngram):\n",
      "    print(skipgram.tostring(classdecoder), \" -- occurring \", occurrences, \" times\" )\n",
      "    \n",
      "#(TODO: a bug in colibri core has to be solved for this to work as it should)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "to the {*1*} of  -- occurring  8  times\n",
        "to {*2*} of  -- occurring  8  times\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another trait of indexed pattern models is the ability to extract co-occurrence information using the ``getcooc()`` method. Let's see with what patterns the ngram \"the law of\" co-occurs more than five times (the second argument specifies this threshold, using it is always more efficient than doing a check on the variable ``occurrences`` that is returned):\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram = classencoder.buildpattern(\"the law\")\n",
      "\n",
      "for coocngram, occurrences in sorted( model.getcooc(ngram,5), key=lambda x: x[1] *-1): #let's sort the output too\n",
      "    print(coocngram.tostring(classdecoder), \" -- occurring \", occurrences, \" times\")\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ",  -- occurring  99  times\n",
        "the  -- occurring  97  times\n",
        "and  -- occurring  75  times\n",
        "of  -- occurring  72  times\n",
        "to  -- occurring  45  times\n",
        "in  -- occurring  34  times\n",
        "is  -- occurring  34  times\n",
        ".  -- occurring  32  times\n",
        "a  -- occurring  30  times\n",
        "the {*1*} of  -- occurring  28  times\n",
        ", and  -- occurring  24  times\n",
        ";  -- occurring  24  times\n",
        "which  -- occurring  21  times\n",
        "or  -- occurring  19  times\n",
        "they  -- occurring  19  times\n",
        "their  -- occurring  18  times\n",
        "not  -- occurring  18  times\n",
        "by  -- occurring  16  times\n",
        "that  -- occurring  15  times\n",
        "be  -- occurring  15  times\n",
        "the {*2*} the  -- occurring  14  times\n",
        "have  -- occurring  14  times\n",
        "of the  -- occurring  14  times\n",
        "them  -- occurring  13  times\n",
        "the {*1*} ,  -- occurring  12  times\n",
        "are  -- occurring  12  times\n",
        "he  -- occurring  12  times\n",
        "this  -- occurring  11  times\n",
        ", {*3*} ,  -- occurring  10  times\n",
        "the {*6*} the  -- occurring  10  times\n",
        ", {*1*} the  -- occurring  9  times\n",
        "as  -- occurring  9  times\n",
        "; and  -- occurring  9  times\n",
        ", the  -- occurring  9  times\n",
        "we  -- occurring  9  times\n",
        "his  -- occurring  8  times\n",
        "but  -- occurring  8  times\n",
        "man  -- occurring  8  times\n",
        "the {*4*} ,  -- occurring  8  times\n",
        "in the  -- occurring  8  times\n",
        "the {*1*} of the  -- occurring  8  times\n",
        ", {*1*} ,  -- occurring  8  times\n",
        "these  -- occurring  8  times\n",
        "and {*3*} the  -- occurring  8  times\n",
        "when  -- occurring  8  times\n",
        "and {*1*} ,  -- occurring  8  times\n",
        "of {*5*} the  -- occurring  8  times\n",
        ", {*4*} and  -- occurring  7  times\n",
        "for  -- occurring  7  times\n",
        "to be  -- occurring  7  times\n",
        "I  -- occurring  7  times\n",
        ", {*3*} to  -- occurring  7  times\n",
        "of a  -- occurring  7  times\n",
        "And  -- occurring  7  times\n",
        "is {*3*} the  -- occurring  7  times\n",
        ", {*3*} of  -- occurring  7  times\n",
        ", {*3*} the  -- occurring  7  times\n",
        "is not  -- occurring  7  times\n",
        "of {*1*} and  -- occurring  6  times\n",
        "of these  -- occurring  6  times\n",
        "the {*3*} and  -- occurring  6  times\n",
        "of {*2*} ,  -- occurring  6  times\n",
        ", {*5*} the  -- occurring  6  times\n",
        "by the  -- occurring  6  times\n",
        "and {*3*} and  -- occurring  6  times\n",
        "of {*4*} ,  -- occurring  6  times\n",
        "the {*6*} ,  -- occurring  6  times\n",
        "--  -- occurring  6  times\n",
        "him  -- occurring  6  times\n",
        "to the  -- occurring  6  times\n",
        "of {*1*} ,  -- occurring  6  times\n",
        "with  -- occurring  6  times\n",
        "there  -- occurring  6  times\n",
        ", or  -- occurring  6  times\n",
        "the {*4*} and  -- occurring  6  times\n",
        "of {*2*} the  -- occurring  6  times\n",
        "in {*2*} ,  -- occurring  6  times\n",
        "her  -- occurring  5  times\n",
        "is the  -- occurring  5  times\n",
        ", {*2*} to  -- occurring  5  times\n",
        "state  -- occurring  5  times\n",
        "own  -- occurring  5  times\n",
        "they have  -- occurring  5  times\n",
        ", {*3*} and  -- occurring  5  times\n",
        "the {*2*} of  -- occurring  5  times\n",
        "has  -- occurring  5  times\n",
        ", {*2*} of  -- occurring  5  times\n",
        "who  -- occurring  5  times\n",
        "the {*5*} the  -- occurring  5  times\n",
        "no  -- occurring  5  times\n",
        "one  -- occurring  5  times\n",
        "of {*3*} ,  -- occurring  5  times\n",
        "in which  -- occurring  5  times\n",
        ", {*2*} and  -- occurring  5  times\n",
        "of {*4*} and  -- occurring  5  times\n",
        "State  -- occurring  5  times\n",
        "what  -- occurring  5  times\n",
        "the {*3*} ,  -- occurring  5  times\n",
        ", {*2*} the  -- occurring  5  times\n",
        "all  -- occurring  5  times\n",
        ", {*5*} ,  -- occurring  5  times\n",
        "'  -- occurring  5  times\n",
        "the {*5*} ,  -- occurring  5  times\n",
        "father  -- occurring  5  times\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are also specific methods for extracting co-occurrences left or right of the pattern: ``getleftcooc()`` and ``getrightcooc()``. Other relationships can be extracted in an identical fashion:\n",
      "\n",
      " * **``getleftneighbours``**``(pattern,threshold=0,category=0,size=0)`` -- returns the neighbours to the immediate left of a pattern (threshold, category and size are constraints which are set to 0 by default)\n",
      " * **``getrightneighbours``**``(pattern,threshold=0,category=0,size=0)``-- returns the neighbours to the immediate right of a pattern\n",
      " * **``getsubchildren``**``(pattern,threshold=0,category=0,size=0)``-- returns patterns that are a subpart (subsumed by) the specified\n",
      " * **``getsubparents``**``(pattern,threshold=0,category=0,size=0)``-- the reverse of the above, returns patterns which subsume the specified patterns"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(TODO: this section is be continued later with flexgrams)*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Comparing pattern models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pattern Models can be used in a train/test paradigm. You can create a Pattern Model on the training corpus and then generated a Pattern Model on the test corpus **constrained** by the training model. This allows you to test what patterns from the training corpus also occur in the test corpus, and how often. Statistics on these two differing counts can provide insight into how much corpora differ.\n",
      "\n",
      "We already saw the *coverage* metric previously, when applied to a train/test scenario it measures the number or ratio of tokens in the test corpus covered by patterns found during training. Let's perform such a comparison.\n",
      "\n",
      "We made a Pattern Model on Plato's Republic and we have a small excerpt from Hamlet. Let's use the former as training and the letter as test."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When doing any kind of comparison, it is absolutely crucial that you make sure the training and test data are class encoded with the same classes. The best method for this is to build the class files for all data in advance. In the previous class encoding example we saw ``classencoder.build()`` which does nothing more than provide us with a shortcut to call ``classencoder.processcorpus()`` followed by ``classencoder.buildclasses()``. To process multiple corpora, we do this ourselves:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classfile2 = TMPDIR + \"platoandhamlet.colibri.cls\"\n",
      "\n",
      "#Instantiate class encoder\n",
      "classencoder2 = colibricore.ClassEncoder()\n",
      "\n",
      "#Build classes\n",
      "classencoder2.processcorpus(corpusfile_plato_plaintext)\n",
      "classencoder2.processcorpus(corpusfile_plaintext)\n",
      "classencoder2.buildclasses()\n",
      "\n",
      "#Save class file\n",
      "classencoder2.save(classfile2)\n",
      "\n",
      "print(\"Encoded \", len(classencoder2), \" classes, well done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Encoded  11540  classes, well done!\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is important to realise that the Class Encoder we just built  (``classencoder2``) is now not compatible with the earlier class encoder used for previous examples! \n",
      "\n",
      "Often, however, you do not have all data available in advance. You may add a different test set later on, long after training. The way to make sure you have a proper class encoding is to extend your original class encoding. Rather than using the class encoder we just build, let us opt for that method, as this will keep all the classes we already had for the training data (Plato's Republic). This we do by calling the ``encodefile()`` method with two extra arguments set to True, indicating respectively that unknown words are allowed, and that unknown words are automatically added the the class encoding. If the second boolean is set to False, all unknown words would be encoded by one single class reserved for unknown words.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Class encoder has \", len(classencoder), \" classes prior to extension\")\n",
      "\n",
      "testcorpusfile = TMPDIR + \"hamlet_test.colibri.dat\" #this will be the encoded test corpus file\n",
      "classencoder.encodefile(corpusfile_plaintext, testcorpusfile, True, True)\n",
      "\n",
      "classfile_test = TMPDIR + \"platoplushamlet.colibri.cls\"\n",
      "classencoder.save(classfile_test)\n",
      "\n",
      "print(\"Class encoder has \", len(classencoder), \" classes after extension\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Class encoder has  11543  classes prior to extension\n",
        "Class encoder has  11543  classes after extension\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do note that this method of encoding is not optimal, only encoding everything in one go ensures the smallest possible memory footprint."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We already created a pattern model on the training data in one of our earlier steps (called ``model``), to create our test model we *train* a constrained model on the test set, this model is constrained by the training model we made earlier. This will result in a new pattern model. The nomenclature may be a bit confusing at first. We simply do all this by instantiating a new model and calling the ``train()`` method and passing the contraining model as the last argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set the options\n",
      "options = colibricore.PatternModelOptions(mintokens=2,maxlength=8,doreverseindex=True)\n",
      "\n",
      "#Instantiate an empty indexed model \n",
      "testmodel = colibricore.IndexedPatternModel()\n",
      "\n",
      "#Train it on our test corpus file (class-encoded data, not plain text)\n",
      "testmodel.train(testcorpusfile, options, model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a test model (effectively the intersection an unconstrained model of the test corpus and the training model). We can see what patterns from the training corpus occur in the test corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for pattern in testmodel:\n",
      "    print(pattern.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "is\n",
        "in\n",
        "No\n",
        "this\n",
        "death\n",
        "of\n",
        "For\n",
        "that\n",
        "by\n",
        "sleep ,\n",
        "die ,\n",
        "we\n",
        "to sleep\n",
        ", to\n",
        "sleep\n",
        "die\n",
        "death ,\n",
        "and the\n",
        "To\n",
        "and\n",
        "That\n",
        "bear\n",
        ".\n",
        "be\n",
        ",\n",
        "makes\n",
        "to\n",
        "And\n",
        "have\n",
        "not\n",
        "we have\n",
        "be ,\n",
        "the\n",
        "a\n",
        "make\n",
        "us\n",
        "life\n",
        ", and the\n",
        ", and\n",
        "would\n",
        ", the\n",
        "to be\n",
        "With\n",
        "The\n",
        "all\n",
        "When\n",
        "end\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can inspect the differences between the counts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for pattern in testmodel:\n",
      "    print(pattern.tostring(classdecoder), \" ---  in training: \", model.occurrencecount(pattern), \", in test: \", testmodel.occurrencecount(pattern)   )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "is  ---  in training:  4619 , in test:  2\n",
        "in  ---  in training:  4319 , in test:  3\n",
        "No  ---  in training:  85 , in test:  2\n",
        "this  ---  in training:  936 , in test:  2\n",
        "death  ---  in training:  60 , in test:  2\n",
        "of  ---  in training:  10374 , in test:  15\n",
        "For  ---  in training:  169 , in test:  2\n",
        "that  ---  in training:  2830 , in test:  4\n",
        "by  ---  in training:  1324 , in test:  2\n",
        "sleep ,  ---  in training:  3 , in test:  3\n",
        "die ,  ---  in training:  3 , in test:  2\n",
        "we  ---  in training:  1314 , in test:  4\n",
        "to sleep  ---  in training:  4 , in test:  2\n",
        ", to  ---  in training:  131 , in test:  3\n",
        "sleep  ---  in training:  13 , in test:  5\n",
        "die  ---  in training:  13 , in test:  2\n",
        "death ,  ---  in training:  14 , in test:  2\n",
        "and the  ---  in training:  736 , in test:  2\n",
        "To  ---  in training:  88 , in test:  5\n",
        "and  ---  in training:  8517 , in test:  7\n",
        "That  ---  in training:  197 , in test:  3\n",
        "bear  ---  in training:  30 , in test:  3\n",
        ".  ---  in training:  7014 , in test:  5\n",
        "be  ---  in training:  2930 , in test:  3\n",
        ",  ---  in training:  15352 , in test:  36\n",
        "makes  ---  in training:  76 , in test:  2\n",
        "to  ---  in training:  5917 , in test:  9\n",
        "And  ---  in training:  1071 , in test:  5\n",
        "have  ---  in training:  1485 , in test:  2\n",
        "not  ---  in training:  2267 , in test:  2\n",
        "we have  ---  in training:  138 , in test:  2\n",
        "be ,  ---  in training:  23 , in test:  2\n",
        "the  ---  in training:  14783 , in test:  15\n",
        "a  ---  in training:  3924 , in test:  5\n",
        "make  ---  in training:  224 , in test:  2\n",
        "us  ---  in training:  436 , in test:  3\n",
        "life  ---  in training:  395 , in test:  2\n",
        ", and the  ---  in training:  310 , in test:  2\n",
        ", and  ---  in training:  2899 , in test:  2\n",
        "would  ---  in training:  590 , in test:  2\n",
        ", the  ---  in training:  551 , in test:  2\n",
        "to be  ---  in training:  940 , in test:  2\n",
        "With  ---  in training:  9 , in test:  2\n",
        "The  ---  in training:  797 , in test:  6\n",
        "all  ---  in training:  867 , in test:  2\n",
        "When  ---  in training:  78 , in test:  2\n",
        "end  ---  in training:  103 , in test:  2\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This isn't so informative unless we apply some normalisation, so let's get the coverage instead:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for pattern in testmodel:\n",
      "    print(pattern.tostring(classdecoder), \" ---  in training: \", model.coverage(pattern), \", in test: \", testmodel.coverage(pattern)   )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "is  ---  in training:  0.01836383370373757 , in test:  0.006349206349206349\n",
        "in  ---  in training:  0.01717111880633093 , in test:  0.009523809523809525\n",
        "No  ---  in training:  0.0003379358875985481 , in test:  0.006349206349206349\n",
        "this  ---  in training:  0.0037212704799087174 , in test:  0.006349206349206349\n",
        "death  ---  in training:  0.00023854297948132806 , in test:  0.006349206349206349\n",
        "of  ---  in training:  0.04124408115232162 , in test:  0.047619047619047616\n",
        "For  ---  in training:  0.0006718960588724073 , in test:  0.006349206349206349\n",
        "that  ---  in training:  0.011251277198869307 , in test:  0.012698412698412698\n",
        "by  ---  in training:  0.005263848413887972 , in test:  0.006349206349206349\n",
        "sleep ,  ---  in training:  2.3854297948132806e-05 , in test:  0.01904761904761905\n",
        "die ,  ---  in training:  2.3854297948132806e-05 , in test:  0.012698412698412698\n",
        "we  ---  in training:  0.005224091250641084 , in test:  0.012698412698412698\n",
        "to sleep  ---  in training:  3.1805730597510405e-05 , in test:  0.012698412698412698\n",
        ", to  ---  in training:  0.0010416376770684657 , in test:  0.01904761904761905\n",
        "sleep  ---  in training:  5.168431222095441e-05 , in test:  0.015873015873015872\n",
        "die  ---  in training:  5.168431222095441e-05 , in test:  0.006349206349206349\n",
        "death ,  ---  in training:  0.00011132005709128642 , in test:  0.012698412698412698\n",
        "and the  ---  in training:  0.005852254429941915 , in test:  0.012698412698412698\n",
        "To  ---  in training:  0.00034986303657261447 , in test:  0.015873015873015872\n",
        "and  ---  in training:  0.03386117593737452 , in test:  0.022222222222222223\n",
        "That  ---  in training:  0.0007832161159636937 , in test:  0.009523809523809525\n",
        "bear  ---  in training:  0.00011927148974066403 , in test:  0.009523809523809525\n",
        ".  ---  in training:  0.02788567430136725 , in test:  0.015873015873015872\n",
        "be  ---  in training:  0.011648848831338186 , in test:  0.009523809523809525\n",
        ",  ---  in training:  0.06103519701662247 , in test:  0.11428571428571428\n",
        "makes  ---  in training:  0.0003021544406763489 , in test:  0.006349206349206349\n",
        "to  ---  in training:  0.023524313493183634 , in test:  0.02857142857142857\n",
        "And  ---  in training:  0.004257992183741705 , in test:  0.015873015873015872\n",
        "have  ---  in training:  0.005903938742162869 , in test:  0.006349206349206349\n",
        "not  ---  in training:  0.009012948908069512 , in test:  0.006349206349206349\n",
        "we have  ---  in training:  0.001097297705614109 , in test:  0.012698412698412698\n",
        "be ,  ---  in training:  0.00018288295093568484 , in test:  0.012698412698412698\n",
        "the  ---  in training:  0.05877301442787454 , in test:  0.047619047619047616\n",
        "a  ---  in training:  0.015600710858078855 , in test:  0.015873015873015872\n",
        "make  ---  in training:  0.0008905604567302914 , in test:  0.006349206349206349\n",
        "us  ---  in training:  0.0017334123175643172 , in test:  0.009523809523809525\n",
        "life  ---  in training:  0.0015704079482520763 , in test:  0.006349206349206349\n",
        ", and the  ---  in training:  0.003697416181960585 , in test:  0.01904761904761905\n",
        ", and  ---  in training:  0.023051203250545667 , in test:  0.012698412698412698\n",
        "would  ---  in training:  0.0023456726315663925 , in test:  0.006349206349206349\n",
        ", the  ---  in training:  0.0043812393898070585 , in test:  0.012698412698412698\n",
        "to be  ---  in training:  0.0074743466904149455 , in test:  0.012698412698412698\n",
        "With  ---  in training:  3.578144692219921e-05 , in test:  0.006349206349206349\n",
        "The  ---  in training:  0.003168645910776974 , in test:  0.01904761904761905\n",
        "all  ---  in training:  0.0034469460535051905 , in test:  0.006349206349206349\n",
        "When  ---  in training:  0.0003101058733257265 , in test:  0.006349206349206349\n",
        "end  ---  in training:  0.0004094987814429465 , in test:  0.006349206349206349\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Particularly the total coverage may be an interesting metric for similarity accross of corpora, which we can compute as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coverage = testmodel.totaltokensingroup() / testmodel.tokens()\n",
      "\n",
      "print(coverage)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.12380952380952381\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get a more traditional frequency metric for a pattern, you have to be aware that the total that is used in normalisation is impacted by the fact that the model is constrained! It will not include any unseen n-grams, for that you'd need an unconstrained model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sleep = classencoder.buildpattern(\"to sleep\")\n",
      "\n",
      "print(\"Frequency in training:\", model.frequency(sleep))\n",
      "\n",
      "print(\"Frequency in test (constrained):\", testmodel.frequency(sleep) )\n",
      "print(\"Coverage in test (constrained):\", testmodel.coverage(sleep) )\n",
      "\n",
      "fullmodel = colibricore.IndexedPatternModel()\n",
      "fullmodel.train(testcorpusfile, options)\n",
      "print(\"Frequency in test (unconstrained):\", fullmodel.frequency(sleep) )\n",
      "print(\"Coverage in test (unconstrained):\", fullmodel.coverage(sleep) )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Frequency in training: 2.1719535636328094e-05\n",
        "Frequency in test (constrained): 0.08333333333333333\n",
        "Coverage in test (constrained): 0.012698412698412698\n",
        "Frequency in test (unconstrained): 0.07692307692307693\n",
        "Coverage in test (unconstrained): 0.012698412698412698\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}